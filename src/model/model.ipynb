{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from audio_preprocessor import AudioFeatureExtractor\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Load environment variables (if you have any)\n",
    "load_dotenv()\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 10,\n",
    "    'd_model': 256,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward': 512,\n",
    "    'sample_rate': 16000,\n",
    "    'n_mels': 128,\n",
    "    'window_size': 25,  # in milliseconds\n",
    "    'hop_size': 10,  # in milliseconds\n",
    "    'chunk_size': 1024,\n",
    "    'patch_size': 32,\n",
    "    'patch_overlap': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio preprocessor\n",
    "preprocessor = AudioFeatureExtractor(\n",
    "    sample_rate=16000,\n",
    "    n_mels=128,\n",
    "    window_size=25,\n",
    "    hop_size=10,\n",
    "    chunk_size=1024,\n",
    "    patch_size=32,\n",
    "    chunk_overlap=256,\n",
    "    patch_overlap=8,\n",
    ")\n",
    "\n",
    "# Dataset class for loading audio files\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, preprocessor):\n",
    "        self.file_paths = file_paths\n",
    "        self.preprocessor = preprocessor\n",
    "        self.data = []\n",
    "\n",
    "        for file_path in tqdm(self.file_paths, desc=\"Preprocessing data\"):\n",
    "            chunks = self.preprocessor(file_path)\n",
    "            self.data.extend(chunks)  # Add all chunks to the dataset     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float)\n",
    "\n",
    "# Custom collate function to handle variable length audio chunks\n",
    "def collate_fn(batch):\n",
    "    batch = [torch.tensor(item).float() for item in batch]\n",
    "    batch = torch.stack(batch)\n",
    "    return batch\n",
    "\n",
    "# Load data\n",
    "file_paths = [os.path.join('../data/songs', f) for f in os.listdir('../data/songs') if f.endswith('.mp3')]\n",
    "file_paths = file_paths[:100]  # Use only first 10 files for now\n",
    "\n",
    "# Calculate mean and std for dataset\n",
    "mean, std = AudioFeatureExtractor.calculate_dataset_stats(file_paths, preprocessor)\n",
    "\n",
    "preprocessor.mean = mean\n",
    "preprocessor.std = std\n",
    "\n",
    "print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "dataset = AudioDataset(file_paths, preprocessor)\n",
    "\n",
    "# Create train/val split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=HYPERPARAMS['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Total number of audio files: {len(dataset)}\")\n",
    "print(f\"Number of batches in training set: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in validation set: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file_paths[0]\n",
    "features = preprocessor(file_path, do_chunk=True, do_patch=False)\n",
    "print(f\"Chunk, Mel_bin, Time : {np.array(features).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file_paths[0]\n",
    "features = preprocessor(file_path)\n",
    "print(f\"Chunk, Number of Patches, Mel_bin, Time: {np.array(features).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example file\n",
    "file_path = file_paths[0]\n",
    "\n",
    "# Extract spectrogram\n",
    "spec = preprocessor(file_path, do_chunk=False, do_normalize=True)\n",
    "chunk = preprocessor(file_path, do_chunk=True, do_patch=False)[0]\n",
    "patches = preprocessor(file_path, do_chunk=True, do_patch=True)[0] # Get patches for the first chunk\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Full spectogram\", \"First Chunk\"))\n",
    "\n",
    "# Display full spectrogram\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=spec,\n",
    "        colorscale='Viridis',\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Display first chunk\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=chunk,\n",
    "        colorscale='Viridis',\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=f\"Mel Spectrogram for {os.path.basename(file_path)}\",\n",
    "    height=800,  # Adjust height as needed\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update y-axis range to match between subplots\n",
    "fig.update_yaxes(range=[0, spec.shape[0]], row=1, col=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Create a subplot figure with 2 rows and 4 columns\n",
    "fig = make_subplots(rows=2, cols=4, subplot_titles=[f\"Patch {i+1}\" for i in range(4)] + [f\"Patch {i+1}\" for i in range(len(patches) - 4,len(patches))])\n",
    "min_val = np.min([np.min(patch) for patch in patches])\n",
    "max_val = np.max([np.max(patch) for patch in patches])\n",
    "\n",
    "# Display first 8 patches\n",
    "for i in range(4):\n",
    "    patch = patches[i]\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=patch,\n",
    "            colorscale='Viridis',\n",
    "            zmin=min_val,\n",
    "            zmax=max_val,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=(i % 4) + 1\n",
    "    )\n",
    "\n",
    "for i in range(-4,0,1):\n",
    "    patch = patches[i]\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=patch,\n",
    "            colorscale='Viridis',\n",
    "            zmin=min_val,\n",
    "            zmax=max_val,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=(i % 4) + 1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=f\"First/ Last 4 patches for {os.path.basename(file_path)}\",\n",
    "    height=600,  # Adjust height as needed\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioTransformerModel(nn.Module):\n",
    "    def __init__(self, patch_size, num_layers, num_heads, d_model, dim_feedforward, dropout=0.1):\n",
    "        super(AudioTransformerModel, self).__init__()\n",
    "\n",
    "        self.conv2d = nn.Conv2d(in_channels=1, out_channels=d_model, kernel_size=(2, 2), stride=2)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear_proj = nn.Linear(d_model, d_model)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 258, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.output_layer = nn.Linear(d_model, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_patches, mel_bins, time = x.size()\n",
    "        \n",
    "        # Reshape input to (batch_size * num_patches, 1, mel_bins, time)\n",
    "        x = x.view(batch_size * num_patches, 1, mel_bins, time)\n",
    "        \n",
    "        # Apply 2D convolution\n",
    "        x = self.conv2d(x)\n",
    "        x = self.gelu(x)\n",
    "        \n",
    "        # Reshape to (batch_size, num_patches, d_model, new_height * new_width)\n",
    "        _, d_model, new_height, new_width = x.size()\n",
    "        x = x.view(batch_size, num_patches, d_model, -1).mean(dim=-1)\n",
    "        \n",
    "        # Linear projection\n",
    "        x = self.linear_proj(x)\n",
    "\n",
    "        # Layer normalization\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoder[:, :num_patches, :]\n",
    "        \n",
    "        # Transformer encoder\n",
    "        x = self.transformer_encoder(x.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        # Layer normalization\n",
    "        x = self.layer_norm2(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x.mean(dim=1))  # Global average pooling\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = AudioTransformerModel(\n",
    "    patch_size=HYPERPARAMS['patch_size'],\n",
    "    num_layers=HYPERPARAMS['num_layers'],\n",
    "    num_heads=HYPERPARAMS['nhead'],\n",
    "    d_model=HYPERPARAMS['d_model'],\n",
    "    dim_feedforward=HYPERPARAMS['dim_feedforward']\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Plotting function for real-time loss visualization\n",
    "def plot_losses(train_losses, val_losses=None):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(train_losses))),\n",
    "        y=train_losses,\n",
    "        mode='lines',\n",
    "        name='Training Loss'\n",
    "    ))\n",
    "    \n",
    "    if val_losses:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(val_losses))),\n",
    "            y=val_losses,\n",
    "            mode='lines',\n",
    "            name='Validation Loss'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Loss over time',\n",
    "        xaxis_title='Epoch',\n",
    "        yaxis_title='Loss',\n",
    "        legend=dict(x=0.1, y=0.9),\n",
    "        height=600,  # Adjust height as needed\n",
    "        width=800,  # Adjust width as needed\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Split batch into two halves for contrastive learning\n",
    "            half = batch.size(0) // 2\n",
    "            embeddings = model(batch)\n",
    "            emb1, emb2 = embeddings[:half], embeddings[half:]\n",
    "            \n",
    "            # Compute loss\n",
    "            target = torch.ones(half).to(device)  # Positive pairs\n",
    "            loss = criterion(emb1, emb2, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Save statistics for plotting\n",
    "        train_losses.append(epoch_loss/len(train_dataloader))\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                half = batch.size(0) // 2\n",
    "                embeddings = model(batch)\n",
    "                emb1, emb2 = embeddings[:half], embeddings[half:]\n",
    "                target = torch.ones(half).to(device)\n",
    "                loss = criterion(emb1, emb2, target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_losses.append(val_loss/len(val_dataloader))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        plot_losses(train_losses, val_losses)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader):.4f}\")\n",
    "        print()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=HYPERPARAMS['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=HYPERPARAMS['num_epochs'])\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, HYPERPARAMS['num_epochs'])\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), 'audio_embedding_model.pth')\n",
    "print(\"Model saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
