{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (if you have any)\n",
    "load_dotenv()\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Constants for audio processing\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "WINDOW_SIZE = 25  # in milliseconds\n",
    "HOP_SIZE = 10  # in milliseconds\n",
    "CHUNK_SIZE = 512  # reduced from 1024 to better fit RTX 2070 memory\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8  # reduced from 16 to fit in GPU memory\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 2\n",
    "D_MODEL = 128  # reduced from 256\n",
    "NHEAD = 4  # reduced from 8\n",
    "NUM_LAYERS = 2  # reduced from 4\n",
    "DIM_FEEDFORWARD = 256  # reduced from 512\n",
    "\n",
    "# Easy way to change hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'd_model': D_MODEL,\n",
    "    'nhead': NHEAD,\n",
    "    'num_layers': NUM_LAYERS,\n",
    "    'dim_feedforward': DIM_FEEDFORWARD\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of audio files: 651\n",
      "Number of batches: 82\n"
     ]
    }
   ],
   "source": [
    "# Audio preprocessing function\n",
    "def preprocess_audio(file_path):\n",
    "    waveform, original_sample_rate = torchaudio.load(file_path)\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=SAMPLE_RATE)(waveform)\n",
    "    waveform = torch.mean(waveform, dim=0, keepdim=True)  # Convert to mono\n",
    "    mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE, \n",
    "        n_mels=N_MELS, \n",
    "        win_length=int(WINDOW_SIZE / 1000 * SAMPLE_RATE), \n",
    "        hop_length=int(HOP_SIZE / 1000 * SAMPLE_RATE)\n",
    "    )(waveform)\n",
    "    log_mel_spec = torchaudio.transforms.AmplitudeToDB()(mel_spec)\n",
    "    return log_mel_spec.squeeze().numpy()\n",
    "\n",
    "# Dataset class for loading audio files\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        log_mel_spec = preprocess_audio(file_path)\n",
    "        return log_mel_spec\n",
    "\n",
    "# Custom collate function to handle variable length audio chunks\n",
    "def collate_fn(batch):\n",
    "    batch = [torch.tensor(item).float() for item in batch]\n",
    "    batch = nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return batch\n",
    "\n",
    "# Load data\n",
    "file_paths = [os.path.join('./previews', f) for f in os.listdir('./previews') if f.endswith('.mp3')]\n",
    "dataset = AudioDataset(file_paths)\n",
    "dataloader = DataLoader(dataset, batch_size=HYPERPARAMS['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Total number of audio files: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AudioEmbeddingModel(\n",
      "  (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "  (pos_encoder): PositionalEncoding()\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=768, bias=True)\n",
      ")\n",
      "Number of parameters: 462592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nirav\\Desktop\\Code\\GitHub\\spotify-song-embedding\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# Model architecture\n",
    "class AudioEmbeddingModel(nn.Module):\n",
    "    def __init__(self, n_mels, d_model, nhead, num_layers, dim_feedforward, max_len=5000):\n",
    "        super(AudioEmbeddingModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_mels, d_model, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout=0.1)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(d_model, 768)  # Output embedding dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(2, 0, 1)  # (N, C, L) -> (L, N, C)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = AudioEmbeddingModel(\n",
    "    n_mels=N_MELS, \n",
    "    d_model=HYPERPARAMS['d_model'], \n",
    "    nhead=HYPERPARAMS['nhead'], \n",
    "    num_layers=HYPERPARAMS['num_layers'], \n",
    "    dim_feedforward=HYPERPARAMS['dim_feedforward']\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbe801275df346f9993b73c2cce40767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39041a8600be4f549e1e071f9cbe84cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deebcf80b924dae95392dcec21de72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHYPERPARAMS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_embedding_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     24\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 25\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=HYPERPARAMS['learning_rate'])\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Split batch into two halves for contrastive learning\n",
    "            half = batch.size(0) // 2\n",
    "            embeddings = model(batch)\n",
    "            emb1, emb2 = embeddings[:half], embeddings[half:]\n",
    "            \n",
    "            # Compute loss\n",
    "            target = torch.ones(half).to(device)  # Positive pairs\n",
    "            loss = criterion(emb1, emb2, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, dataloader, criterion, optimizer, HYPERPARAMS['num_epochs'])\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), 'audio_embedding_model.pth')\n",
    "print(\"Model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding(model, audio_path):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        log_mel_spec = preprocess_audio(audio_path)\n",
    "        log_mel_spec = torch.tensor(log_mel_spec).unsqueeze(0).float().to(device)\n",
    "        embedding = model(log_mel_spec)\n",
    "    return embedding.cpu().numpy()\n",
    "\n",
    "# Example usage:\n",
    "# new_audio_path = \"path/to/new/audio.mp3\"\n",
    "# embedding = generate_embedding(trained_model, new_audio_path)\n",
    "# print(\"Generated embedding shape:\", embedding.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
