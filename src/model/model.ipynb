{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Datasetr\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from audio_preprocessor import AudioFeatureExtractor\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Load environment variables (if you have any)\n",
    "load_dotenv()\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "HYPERPARAMS = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 1e-6,\n",
    "    'num_epochs': 10,\n",
    "    'd_model': 256,\n",
    "    'nhead': 8,\n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward': 512,\n",
    "    'sample_rate': 16000,\n",
    "    'n_mels': 128,\n",
    "    'window_size': 25,  # in milliseconds\n",
    "    'hop_size': 10,  # in milliseconds\n",
    "    'chunk_size': 1024,\n",
    "    'patch_size': 32,\n",
    "    'patch_overlap': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio preprocessor\n",
    "preprocessor = AudioFeatureExtractor(\n",
    "    sample_rate=16000,\n",
    "    n_mels=128,\n",
    "    window_size=25,\n",
    "    hop_size=10,\n",
    "    chunk_size=1024,\n",
    "    patch_size=32,\n",
    "    chunk_overlap=256,\n",
    "    patch_overlap=8,\n",
    ")\n",
    "\n",
    "# Dataset class for loading audio files\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, preprocessor):\n",
    "        self.file_paths = file_paths\n",
    "        self.preprocessor = preprocessor\n",
    "        self.data = []\n",
    "\n",
    "        for file_path in tqdm(self.file_paths, desc=\"Preprocessing data\"):\n",
    "            chunks = self.preprocessor(file_path)\n",
    "            self.data.extend(chunks)  # Add all chunks to the dataset     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float)\n",
    "\n",
    "# Custom collate function to handle variable length audio chunks\n",
    "def collate_fn(batch):\n",
    "    batch = [torch.tensor(item).float() for item in batch]\n",
    "    batch = torch.stack(batch)\n",
    "    return batch\n",
    "\n",
    "# Load data\n",
    "file_paths = [os.path.join('../data/songs1', f) for f in os.listdir('../data/songs1') if f.endswith('.mp3')]\n",
    "#file_paths = file_paths[:10000]  # Use only first 10 files for now\n",
    "\n",
    "# Calculate mean and std for dataset\n",
    "mean, std = AudioFeatureExtractor.calculate_dataset_stats(file_paths, preprocessor)\n",
    "\n",
    "preprocessor.mean = mean\n",
    "preprocessor.std = std\n",
    "\n",
    "print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "dataset = AudioDataset(file_paths, preprocessor)\n",
    "\n",
    "# Create train/val split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=HYPERPARAMS['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=HYPERPARAMS['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Total number of audio files: {len(dataset)}\")\n",
    "print(f\"Number of batches in training set: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in validation set: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file_paths[0]\n",
    "features = preprocessor(file_path, do_chunk=True, do_patch=False)\n",
    "print(f\"Chunk, Mel_bin, Time : {np.array(features).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file_paths[0]\n",
    "features = preprocessor(file_path)\n",
    "print(f\"Chunk, Number of Patches, Mel_bin, Time: {np.array(features).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example file\n",
    "file_path = file_paths[0]\n",
    "\n",
    "# Extract spectrogram\n",
    "spec = preprocessor(file_path, do_chunk=False, do_normalize=True)\n",
    "chunk = preprocessor(file_path, do_chunk=True, do_patch=False)[0]\n",
    "patches = preprocessor(file_path, do_chunk=True, do_patch=True)[0] # Get patches for the first chunk\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Full spectogram\", \"First Chunk\"))\n",
    "\n",
    "# Display full spectrogram\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=spec,\n",
    "        colorscale='Viridis',\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Display first chunk\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=chunk,\n",
    "        colorscale='Viridis',\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=f\"Mel Spectrogram for {os.path.basename(file_path)}\",\n",
    "    height=800,  # Adjust height as needed\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update y-axis range to match between subplots\n",
    "fig.update_yaxes(range=[0, spec.shape[0]], row=1, col=1)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Create a subplot figure with 2 rows and 4 columns\n",
    "fig = make_subplots(rows=2, cols=4, subplot_titles=[f\"Patch {i+1}\" for i in range(4)] + [f\"Patch {i+1}\" for i in range(len(patches) - 4,len(patches))])\n",
    "min_val = np.min([np.min(patch) for patch in patches])\n",
    "max_val = np.max([np.max(patch) for patch in patches])\n",
    "\n",
    "# Display first 8 patches\n",
    "for i in range(4):\n",
    "    patch = patches[i]\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=patch,\n",
    "            colorscale='Viridis',\n",
    "            zmin=min_val,\n",
    "            zmax=max_val,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=(i % 4) + 1\n",
    "    )\n",
    "\n",
    "for i in range(-4,0,1):\n",
    "    patch = patches[i]\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=patch,\n",
    "            colorscale='Viridis',\n",
    "            zmin=min_val,\n",
    "            zmax=max_val,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=(i % 4) + 1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=f\"First/ Last 4 patches for {os.path.basename(file_path)}\",\n",
    "    height=600,  # Adjust height as needed\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from custom_model import AudioTransformerModel\n",
    "\n",
    "# Initialize the model\n",
    "model = AudioTransformerModel(\n",
    "    patch_size=HYPERPARAMS['patch_size'],\n",
    "    num_layers=HYPERPARAMS['num_layers'],\n",
    "    num_heads=HYPERPARAMS['nhead'],\n",
    "    d_model=HYPERPARAMS['d_model'],\n",
    "    dim_feedforward=HYPERPARAMS['dim_feedforward']\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "# Plotting function for real-time loss visualization\n",
    "def plot_losses(train_losses, val_losses=None, epoch_batches=None, save_path=None):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(train_losses))),\n",
    "        y=train_losses,\n",
    "        mode='lines',\n",
    "        name='Training Loss'\n",
    "    ))\n",
    "    \n",
    "    if val_losses:\n",
    "        val_x = [(i + 1) * epoch_batches - 1 for i in range(len(val_losses))]\n",
    "        val_x = [0] + val_x  # Ensure validation loss starts from batch 0\n",
    "        val_losses = [val_losses[0]] + val_losses  # Duplicate the first validation loss for batch 0\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=val_x,\n",
    "            y=val_losses,\n",
    "            mode='lines',\n",
    "            name='Validation Loss'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Training and Validation Loss over time',\n",
    "        xaxis_title='Batch',\n",
    "        yaxis_title='Loss',\n",
    "        legend=dict(x=0.1, y=0.9),\n",
    "        height=600,  # Adjust height as needed\n",
    "        width=800,  # Adjust width as needed\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        fig.write_image(save_path)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "def create_triplets(embeddings, batch_size):\n",
    "    # Ensure the batch size is even\n",
    "    if batch_size % 2 != 0:\n",
    "        embeddings = embeddings[:-1]\n",
    "        batch_size -= 1\n",
    "    \n",
    "    # Split batch into anchors and positives\n",
    "    anchors = embeddings[:batch_size//2]\n",
    "    positives = embeddings[batch_size//2:]\n",
    "    \n",
    "    # Create negative samples by rolling the positives\n",
    "    negatives = torch.roll(positives, shifts=1, dims=0)\n",
    "    \n",
    "    return anchors, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, temperature=0.5):\n",
    "    criterion = TripletLoss(margin=1.0)\n",
    "    train_losses = []\n",
    "    avg_val_losses = []\n",
    "    num_batches = len(train_dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            embeddings = model(batch).squeeze()\n",
    "            anchors, positives, negatives = create_triplets(embeddings, batch.size(0))\n",
    "            \n",
    "            loss = criterion(anchors, positives, negatives)\n",
    "            \n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            # Append the current batch loss to train_losses\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Plot training loss after each batch\n",
    "            plot_losses(train_losses, avg_val_losses, num_batches)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Training Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Calculate and save the average training loss for the epoch\n",
    "        avg_epoch_train_loss = epoch_train_loss / num_batches\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                embeddings = model(batch).squeeze()\n",
    "                anchors, positives, negatives = create_triplets(embeddings, batch.size(0))\n",
    "                \n",
    "                loss = criterion(anchors, positives, negatives)\n",
    "                epoch_val_loss += loss.item()\n",
    "        \n",
    "        # Calculate and save the average validation loss for the epoch\n",
    "        avg_epoch_val_loss = epoch_val_loss / len(val_dataloader)\n",
    "        avg_val_losses.append(avg_epoch_val_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Plot both training and validation losses after each epoch\n",
    "        plot_losses(train_losses, avg_val_losses, num_batches)\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        os.makedirs('saved_models', exist_ok=True)\n",
    "        torch.save(model.state_dict(), f'saved_models/audio_embedding_model_epoch_{epoch+1}.pth')\n",
    "        print(f\"Model saved successfully for epoch {epoch+1}.\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Avg Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_epoch_val_loss:.4f}\")\n",
    "\n",
    "    # Save the final plot\n",
    "    plot_losses(train_losses, avg_val_losses, num_batches, save_path='loss_plot/final_loss_plot.png')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = AudioTransformerModel(\n",
    "    patch_size=HYPERPARAMS['patch_size'],\n",
    "    num_layers=HYPERPARAMS['num_layers'],\n",
    "    num_heads=HYPERPARAMS['nhead'],\n",
    "    d_model=HYPERPARAMS['d_model'],\n",
    "    dim_feedforward=HYPERPARAMS['dim_feedforward']\n",
    ").to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# Loss function and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=HYPERPARAMS['learning_rate'])\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=HYPERPARAMS['num_epochs'])\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, HYPERPARAMS['num_epochs'])\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), 'audio_embedding_model.pth')\n",
    "print(\"Model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
